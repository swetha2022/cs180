<!DOCTYPE html>
<html>
<head>
  <title>Project 2: Fun with Filters and Frequencies</title>
  <style>
    body {
      font-family: Georgia, serif;
      margin: 30px;
      line-height: 1.6;
    }
    h1, h2, h3 {
      text-align: center;
    }
    .flex-center {
      display: flex;
      justify-content: center;
      align-items: flex-end; 
      gap: 20px;
      flex-wrap: wrap;
      margin: 20px 0;
    }
    figure {
      margin: 0;
      width: 20%;
    }
    figure img {
      width: 100%;
      border-radius: 8px;
      box-shadow: 0px 2px 8px rgba(0,0,0,0.2);
    }
    figcaption {
      font-size: 13px;
      text-align: center;
      margin-top: 6px;
    }
    p {
      text-align: justify;
    }
    code {
      background: #eee;
      padding: 2px 5px;
      border-radius: 4px;
      font-family: monospace;
    }
    .matrix {
      display: grid;
      gap: 20px;
      margin: 20px 0;
    }
    .matrix-4 { grid-template-columns: repeat(4, 1fr); }
    .matrix-5 { grid-template-columns: repeat(5, 1fr); }
    .matrix-3 { grid-template-columns: repeat(3, 1fr); }
    .matrix-7 { grid-template-columns: repeat(7, 1fr); }
  </style>
</head>
<body>
  <h1>Project 2: Fun with Filters and Frequencies</h1>
  <h3>Webpage Link: <a href="https://swetha2022.github.io/cs180/2/index.html">https://swetha2022.github.io/cs180/2/index.html</a></h3>

  <h2>Introduction</h2>
  <p>
    In this project, I implemented a variety of classic image processing techniques that explore convolution, edge detection, frequency filtering, and multi-resolution blending. These experiments build intuition on how images can be decomposed into frequency components and recombined to produce compelling effects such as sharpening, hybrid images, and seamless blending.
  </p>

  <!-- ====================== PART 1 ====================== -->
  <h2>Part 1: Filters and Edges</h2>

  <!-- Task 1.1 -->
  <h3>Part 1.1: Convolutions from Scratch!</h3>
  <p>
    In part 1.1, I implemented 2D convolution on an image of myself using three approaches: a four-loop version, a two-loop version with vectorized inner operations, and the built-in <code>scipy.signal.convolve2d</code>. In the four-loop method, I explicitly loop over each pixel of the image and each element of the filter to compute the weighted sum. The two-loop version is faster because the inner loops over the filter are replaced with vectorized operations using NumPy, meaning that entire slices of the image and filter are multiplied and summed at once without explicit Python loops. For small kernels, performance differences between the two-loop and four-loop methods are minor, but as kernel size increases, vectorized and optimized implementations become much more efficient. The built-in function is highly optimized and significantly faster, especially for larger images or kernels. All methods handle boundaries using zero padding to ensure the output image maintains the same size as the input. Using these methods, I convolved the image of myself with a 9×9 box filter as well as finite difference operators <code>Dx</code> and <code>Dy</code>, which capture vertical and horizontal edges, respectively.
  </p>

  <div class="flex-center">
    <figure><img src="./task_1_1/4_loops_code.png" style="width:300px;"><figcaption>Convolution Code (4 loops)</figcaption></figure>
    <figure><img src="./task_1_1/2_loops_code.png" style="width:300px;"><figcaption>Convolution Code (2 loops)</figcaption></figure>
    <figure><img src="./task_1_1/built_in_code.png" style="width:300px;"><figcaption>Convolution Code (built-in)</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_1_1/swetha.jpg"><figcaption>Original</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_1_1/task_1_1_convolved_4_loops_box.png"><figcaption>Box Filter (4 loops)<br>Time Taken: 9.89s</figcaption></figure>
    <figure><img src="./task_1_1/task_1_1_convolved_2_loops_box.png"><figcaption>Box Filter (2 loops)<br>Time Taken: 1.21s</figcaption></figure>
    <figure><img src="./task_1_1/task_1_1_convolved_built_in_box.png"><figcaption>Box Filter (built-in)<br>Time Taken: 0.66s</figcaption></figure>
  </div>
  
  <div class="flex-center">
    <figure><img src="./task_1_1/task_1_1_convolved_4_loops_dx.png"><figcaption>Dx Filter (4 loops)<br>Time Taken: 0.61s</figcaption></figure>
    <figure><img src="./task_1_1/task_1_1_convolved_2_loops_dx.png"><figcaption>Dx Filter (2 loops)<br>Time Taken: 1.16s</figcaption></figure>
    <figure><img src="./task_1_1/task_1_1_convolved_built_in_dx.png"><figcaption>Dx Filter (built-in)<br>Time Taken: 0.0048s</figcaption></figure>
  </div>
  
  <div class="flex-center">
    <figure><img src="./task_1_1/task_1_1_convolved_4_loops_dy.png"><figcaption>Dy Filter (4 loops)<br>Time Taken: 0.72s</figcaption></figure>
    <figure><img src="./task_1_1/task_1_1_convolved_2_loops_dy.png"><figcaption>Dy Filter (2 loops)<br>Time Taken: 1.17s</figcaption></figure>
    <figure><img src="./task_1_1/task_1_1_convolved_built_in_dy.png"><figcaption>Dy Filter (built-in)<br>Time Taken: 0.0060s</figcaption></figure>
  </div>

  <!-- Task 1.2 -->
  <h3>Part 1.2: Finite Difference Operator</h3>
  <p>
    In part 1.2, I focused on the cameraman image. Below are images representing the partial derivatives in the x and y directions, the gradient magnitude image, and a binarized edge image. I used a threshold of 0.3 to create the binarized edge image, chosen qualitatively to balance edge detection and noise removal. Increasing the threshold reduces noise but also removes parts of the cameraman's edges, while decreasing the threshold preserves more edges but allows more background noise. A threshold of 0.3 provided the best compromise between capturing the cameraman's edges and minimizing noise from the background.
  </p>
  <div class="flex-center">
    <figure><img src="./task_1_2/cameraman.png"><figcaption>Cameraman Original</figcaption></figure>
    <figure><img src="./task_1_2/task_1_2_convolved_dx.png"><figcaption>Convolved Dx</figcaption></figure>
    <figure><img src="./task_1_2/task_1_2_convolved_dy.png"><figcaption>Convolved Dy</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_1_2/task_1_2_gradient_magnitude.png"><figcaption>Gradient Magnitude</figcaption></figure>
    <figure><img src="./task_1_2/task_1_2_edge_map.png"><figcaption>Edge Map (Threshold=0.3)</figcaption></figure>
  </div>

  <!-- Task 1.3 -->
  <h3>Part 1.3: Derivative of Gaussian (DoG)</h3>
  <p>
    In part 1.3, I constructed a Gaussian filter using <code>cv2.getGaussianKernel</code> and built a 2D kernel by taking the outer product of the 1D Gaussian kernel with its transpose. Applying this filter to the original grayscale image produced a blurred version, after which I computed the partial derivatives in x and y, the gradient magnitude image, and a binarized edge image. Compared to the finite difference method, the edge map is much smoother and less noisy. I used a threshold of 0.1, chosen qualitatively to balance edge detection and noise suppression. I also implemented derivative-of-Gaussian (DoG) filters by convolving the Gaussian filter with the Dx and Dy filters, and then applied these DoG filters directly to the grayscale image to compute the partial derivatives, gradient magnitude, and binarized edge image. This produced results consistent with the previous Gaussian-smoothing approach. Overall, while the simple difference operator yields noisy edges, combining it with Gaussian smoothing or using DoG filters provides cleaner and smoother edge maps.
  </p>
  <div class="flex-center">
    <figure><img src="./task_1_3/cameraman.png"><figcaption>Original</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_gaussian_filter.png"><figcaption>Gaussian Filter</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_convolved_smoothed_dx.png"><figcaption>Smoothed Dx</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_convolved_smoothed_dy.png"><figcaption>Smoothed Dy</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_gradient_magnitude_smoothed.png"><figcaption>Smoothed Gradient Magnitude</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_edge_map.png"><figcaption>Edge Map (Threshold=0.1)</figcaption></figure>
  </div>  

  <div class="flex-center">
    <figure><img src="./task_1_3/task_1_3_dog_x.png"><figcaption>DoG x Filter</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_dog_y.png"><figcaption>DoG y Filter</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_dog_convolved_smoothed_dx.png"><figcaption>Smoothed Dx</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_dog_convolved_smoothed_dy.png"><figcaption>Smoothed Dy</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_dog_gradient_magnitude_smoothed.png"><figcaption>Smoothed Gradient Magnitude</figcaption></figure>
    <figure><img src="./task_1_3/task_1_3_dog_edge_map.png"><figcaption>Edge Map (Threshold=0.1)</figcaption></figure>
  </div>  

  <!-- ====================== PART 2 ====================== -->
  <h2>Part 2: Applications of Filtering</h2>

  <!-- Task 2.1 -->
  <h3>Part 2.1: Image Sharpening</h3>
  <p>
    In part 2.1, I sharpened a blurry image of the Taj Mahal using the unsharp mask technique. This method works by first creating a blurred version of the original image using a Gaussian filter, which acts as a low-pass filter and retains only the low-frequency components. Subtracting the blurred image from the original isolates the high-frequency components, which correspond to edges and fine details. By adding a scaled version of these high frequencies back to the original image, we enhance the sharpness. Mathematically, this can be expressed as applying the filter <code>(1 + alpha) * unit_impulse - alpha * gaussian_filter</code>, where <code>alpha</code> controls the sharpening amount. Higher values of <code>alpha</code> amplify the high-frequency components more strongly, resulting in a sharper image, while smaller values preserve more of the original blur. I applied this technique to a blurred Taj Mahal, as well as to images of the Statue of Liberty and Eiffel Tower by first blurring the original images and then applying unsharp masking. For the Statue of Liberty and Eiffel Tower, the sharpened images more closely approximated the originals, but some fine details lost during the blurring process could not be fully recovered. Increasing <code>alpha</code> consistently improved the approximation of the original image’s high-frequency content, producing sharper results.
  </p>
  
  <div class="flex-center">
    <figure><img src="./task_2_1/taj.jpg"><figcaption>Taj Original</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_taj_blurred_image.png"><figcaption>Taj Blurred</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_taj_high_frequencies_image.png"><figcaption>Taj High Frequencies</figcaption></figure>
  </div>  

  <div class="flex-center">
    <figure><img src="./task_2_1/task_2_1_taj_sharpened_image_0.5.png"><figcaption>Taj Sharpened (alpha=0.5)</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_taj_sharpened_image_1.png"><figcaption>Taj Sharpened (alpha=1)</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_taj_sharpened_image_2.png"><figcaption>Taj Sharpened (alpha=2)</figcaption></figure>
  </div>  

  <div class="flex-center">
    <figure><img src="./task_2_1/statue.jpg"><figcaption>Statue Original</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_statue_1_blurred_image.png"><figcaption>Statue Blurred</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_statue_1_high_frequencies_image.png"><figcaption>Statue High Frequencies</figcaption></figure>
  </div>  

  <div class="flex-center">
    <figure><img src="./task_2_1/task_2_1_statue_0.5_sharpened_image.png"><figcaption>Statue Sharpened (alpha=0.5)</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_statue_1_sharpened_image.png"><figcaption>Statue Sharpened (alpha=1)</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_statue_2_sharpened_image.png"><figcaption>Statue Sharpened (alpha=2)</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_statue_5_sharpened_image.png"><figcaption>Statue Sharpened (alpha=5)</figcaption></figure>
  </div>  

  <div class="flex-center">
    <figure><img src="./task_2_1/eiffel_tower.jpg"><figcaption>Eiffel Original</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_eiffel_tower_1_blurred_image.png"><figcaption>Eiffel Blurred</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_eiffel_tower_1_high_frequencies_image.png"><figcaption>Eiffel High Frequencies</figcaption></figure>
  </div>  

  <div class="flex-center">
    <figure><img src="./task_2_1/task_2_1_eiffel_tower_0.5_sharpened_image.png"><figcaption>Eiffel Sharpened (alpha=0.5)</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_eiffel_tower_1_sharpened_image.png"><figcaption>Eiffel Sharpened (alpha=1)</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_eiffel_tower_2_sharpened_image.png"><figcaption>Eiffel Sharpened (alpha=2)</figcaption></figure>
    <figure><img src="./task_2_1/task_2_1_eiffel_tower_5_sharpened_image.png"><figcaption>Eiffel Sharpened (alpha=5)</figcaption></figure>
  </div> 

  <!-- Task 2.2 -->
  <h3>Part 2.2: Hybrid Images</h3>
  <p>
    In part 2.2, I created hybrid images by combining two input images in the frequency domain. First, I aligned the two images, then extracted a low-frequency version of the first image and a high-frequency version of the second image. Adding these together produced the final hybrid image. I demonstrated this process in detail on the Derek and Nutmeg images, showing the aligned images, the filtered results, and the final hybrid image, along with the Fourier transforms of all these images. The Fourier transforms were visualized using <code>np.log(np.abs(np.fft.fftshift(np.fft.fft2(gray_image))))</code>. Additionally, I applied the same procedure to a tiger and monkey pair and an alligator and snake pair, showing the original images and their final hybrid results. For all the image pairs below, I used a Gaussian filter with <code>sigma = 8</code> to extract the low-frequency components from the first image, and a Gaussian filter with <code>sigma = 15</code> to extract the high-frequency components from the second image. These values were chosen qualitatively to balance the contribution of low and high frequencies in the final hybrid images, ensuring that the hybrid effect is visually clear without excessive blurring or noise.
  </p> 

  <div class="flex-center">
    <figure><img src="./task_2_2/DerekPicture.jpg"><figcaption>Derek Original</figcaption></figure>
    <figure><img src="./task_2_2/nutmeg.jpg"><figcaption>Nutmeg Original</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_hybrid_image.png"><figcaption>Hybrid</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_im1_aligned.png"><figcaption>Derek Aligned</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_low_frequencies_im1.png"><figcaption>Derek Low Frequencies</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_im2_aligned.png"><figcaption>Nutmeg Aligned</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_high_frequencies_im2.png"><figcaption>Nutmeg High Frequencies</figcaption></figure>
  </div>
  
  <div class="flex-center">
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_fft_im1.png"><figcaption>Derek FFT</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_fft_im2.png"><figcaption>Nutmeg FFT</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_fft_aligned_im1.png"><figcaption>Derek Aligned FFT</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_fft_aligned_im2.png"><figcaption>Nutmeg Aligned FFT</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_fft_low_image_1.png"><figcaption>Derek Low Frequencies FFT</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_fft_high_image_2.png"><figcaption>Nutmeg High Frequencies FFT</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_derek_nutmeg_fft_hybrid.png"><figcaption>Hybrid FFT</figcaption></figure>
  </div>  

  <div class="flex-center">
    <figure><img src="./task_2_2/tiger.jpg"><figcaption>Tiger Original</figcaption></figure>
    <figure><img src="./task_2_2/monkey.jpg"><figcaption>Monkey Original</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_tiger_monkey_hybrid_image.png"><figcaption>Hybrid</figcaption></figure>
  </div>
  
  <div class="flex-center">
    <figure><img src="./task_2_2/alligator.jpg"><figcaption>Alligator Original</figcaption></figure>
    <figure><img src="./task_2_2/snake.jpg"><figcaption>Snake Original</figcaption></figure>
    <figure><img src="./task_2_2/task_2_2_alligator_snake_hybrid_image.png"><figcaption>Hybrid</figcaption></figure>
  </div>  

  <!-- Task 2.3 -->
  <h3>Part 2.3: Gaussian and Laplacian Stacks</h3>
  <p>
    In part 2.3, I created Gaussian and Laplacian stacks for the apple and orange images, implemented over 5 levels. The Gaussian stacks were built by repeatedly applying a Gaussian filter to the previous level, starting with the original image at the first level. The Laplacian stacks were obtained by taking the difference between consecutive levels of the Gaussian stack, with the last level of the Laplacian stack equal to the last level of the Gaussian stack. The visuals of the Gaussian and Laplacian stacks for the two images are demonstrated below.
  </p>

  <!-- Level 0 -->
  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img1_level_0.png"><figcaption>Apple Gaussian<br>Level 0</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img2_level_0.png"><figcaption>Orange Gaussian<br>Level 0</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img1_level_0.png"><figcaption>Apple Laplacian<br>Level 0</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img2_level_0.png"><figcaption>Orange Laplacian<br>Level 0</figcaption></figure>
  </div>

  <!-- Level 1 -->
  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img1_level_1.png"><figcaption>Apple Gaussian<br>Level 1</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img2_level_1.png"><figcaption>Orange Gaussian<br>Level 1</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img1_level_1.png"><figcaption>Apple Laplacian<br>Level 1</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img2_level_1.png"><figcaption>Orange Laplacian<br>Level 1</figcaption></figure>
  </div>

  <!-- Level 2 -->
  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img1_level_2.png"><figcaption>Apple Gaussian<br>Level 2</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img2_level_2.png"><figcaption>Orange Gaussian<br>Level 2</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img1_level_2.png"><figcaption>Apple Laplacian<br>Level 2</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img2_level_2.png"><figcaption>Orange Laplacian<br>Level 2</figcaption></figure>
  </div>

  <!-- Level 3 -->
  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img1_level_3.png"><figcaption>Apple Gaussian<br>Level 3</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img2_level_3.png"><figcaption>Orange Gaussian<br>Level 3</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img1_level_3.png"><figcaption>Apple Laplacian<br>Level 3</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img2_level_3.png"><figcaption>Orange Laplacian<br>Level 3</figcaption></figure>
  </div>

  <!-- Level 4 -->
  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img1_level_4.png"><figcaption>Apple Gaussian<br>Level 4</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_gaussian_img2_level_4.png"><figcaption>Orange Gaussian<br>Level 4</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img1_level_4.png"><figcaption>Apple Laplacian<br>Level 4</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_laplacian_img2_level_4.png"><figcaption>Orange Laplacian<br>Level 4</figcaption></figure>
  </div>

  <!-- Task 2.4 -->
  <h3>Part 2.4: Multiresolution Blending (Oraple)</h3>
  <p>
    In part 2.4, I used the Laplacian stacks from the previous part to implement multi-resolution blending for the apple and orange images. At each level, the Laplacian for the apple was multiplied by the seam, and the Laplacian for the orange was multiplied by <code>1 - seam</code>. The weighted contributions were then combined to produce the blended result at that level. The weighted Laplacians for the apple and orange, along with their combined blend, are shown for levels 0, 2, and 4. The sum of weighted Laplacians for the apple and orange across all levels and the final blended image are shown in the last row. I applied the same procedure to a blue and green notebook pair using a horizontal seam, and to a hand and eye pair using a circular seam, demonstrating that multi-resolution blending can effectively handle a variety of seam shapes.
  </p>

  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/apple.jpeg"><figcaption>Apple</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/orange.jpeg"><figcaption>Orange</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_weighted_img1_level_0.png"><figcaption>Level 0</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_weighted_img2_level_0.png"><figcaption>Level 0</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_blended_level_0.png"><figcaption>Level 0</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_weighted_img1_level_2.png"><figcaption>Level 2</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_weighted_img2_level_2.png"><figcaption>Level 2</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_blended_level_2.png"><figcaption>Level 2</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_weighted_img1_level_4.png"><figcaption>Level 4</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_weighted_img2_level_4.png"><figcaption>Level 4</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_blended_level_4.png"><figcaption>Level 4</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_final_weighted_img1.png"><figcaption>Final Apple Blend</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_final_weighted_img2.png"><figcaption>Final Orange Blend</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_final_blended.png"><figcaption>Final Blend</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/green_notebook.jpg"><figcaption>Green Notebook</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/blue_notebook.jpg"><figcaption>Blue Notebook</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_notebook_final_blended.png"><figcaption>Final Blend</figcaption></figure>
  </div>

  <div class="flex-center">
    <figure><img src="./task_2_3_and_2_4/hand.png"><figcaption>Hand</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/eye.png"><figcaption>Eye</figcaption></figure>
    <figure><img src="./task_2_3_and_2_4/task_2_3_and_2_4_hand_eye_final_blended.png"><figcaption>Final Blend</figcaption></figure>
  </div>

  <h2>Reflection</h2>
  <p>
    One important lesson I learned is the importance of normalizing images before visualizing them. Without normalization, many results appeared either too dark or too bright, making it difficult to interpret the true effect of the filters and transformations.
  </p>

</body>
</html>